{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from crewai_tools import tool\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "\n",
    "from crewai import Task, Agent, Crew\n",
    "from textwrap import dedent\n",
    "\n",
    "# If you have these custom modules:\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    A configuration class that fetches environment variables.\n",
    "    \"\"\"\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "    SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "    LINKEDIN_EMAIL = os.getenv(\"LINKEDIN_EMAIL\")\n",
    "    LINKEDIN_PASSWORD = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "    LINKEDIN_PROFILE_NAME = os.getenv(\"LINKEDIN_PROFILE_NAME\")\n",
    "    ACCESS_TOKEN = os.getenv(\"ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "### Utilities & Exceptions\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedinToolException(Exception):\n",
    "    \"\"\"\n",
    "    Custom exception used when LinkedIn credentials are not provided in env variables.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\"You need to set the LINKEDIN_EMAIL and LINKEDIN_PASSWORD env variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_content(page_source: str):\n",
    "    \"\"\"\n",
    "    Parses the page source HTML of a LinkedIn profile and filters \n",
    "    the containers that contain post information.\n",
    "    \"\"\"\n",
    "    linkedin_soup = BeautifulSoup(page_source.encode(\"utf-8\"), \"lxml\")\n",
    "    containers = linkedin_soup.find_all(\"div\", {\"class\": \"feed-shared-update-v2\"})\n",
    "    containers = [container for container in containers \n",
    "                  if 'activity' in container.get('data-urn', '')]\n",
    "    return containers\n",
    "\n",
    "\n",
    "def get_post_content(container, selector, attributes):\n",
    "    \"\"\"\n",
    "    Retrieves the text content from a specific HTML element \n",
    "    within a container.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = container.find(selector, attributes)\n",
    "        if element:\n",
    "            return element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting post content: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_linkedin_posts(page_source: str):\n",
    "    \"\"\"\n",
    "    Uses parse_html_content to identify relevant containers, then \n",
    "    extracts the post text from each container.\n",
    "    \"\"\"\n",
    "    containers = parse_html_content(page_source)\n",
    "    posts = []\n",
    "    for container in containers:\n",
    "        post_content = get_post_content(container, \"div\", {\"class\": \"update-components-text\"})\n",
    "        posts.append(post_content)\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------\n",
    "#### LinkedIn Scraper Tool\n",
    "### -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_posts_fn() -> str:\n",
    "    \"\"\"\n",
    "    A function that logs into LinkedIn using credentials from environment \n",
    "    variables, scrolls through a profile's posts, and returns the posts.\n",
    "    \"\"\"\n",
    "    linkedin_username = os.environ.get(\"LINKEDIN_EMAIL\")\n",
    "    linkedin_password = os.environ.get(\"LINKEDIN_PASSWORD\")\n",
    "    linkedin_profile_name = os.environ.get(\"LINKEDIN_PROFILE_NAME\")\n",
    "\n",
    "    if not (linkedin_username and linkedin_password):\n",
    "        raise LinkedinToolException()\n",
    "\n",
    "    # Initialize WebDriver (make sure chromedriver is installed and in PATH)\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.linkedin.com/login\")\n",
    "\n",
    "    # Perform login\n",
    "    username_input = browser.find_element(\"id\", \"username\")\n",
    "    password_input = browser.find_element(\"id\", \"password\")\n",
    "    username_input.send_keys(linkedin_username)\n",
    "    password_input.send_keys(linkedin_password)\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Navigate to the profile's \"Recent Activity\"\n",
    "    browser.get(f\"https://www.linkedin.com/in/{linkedin_profile_name}/recent-activity/all/\")\n",
    "\n",
    "    # Scroll to load more posts\n",
    "    for _ in range(2):\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Extract posts\n",
    "    posts = get_linkedin_posts(browser.page_source)\n",
    "    browser.quit()\n",
    "    return str(posts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"ScrapeLinkedinPosts\")\n",
    "def scrape_linkedin_posts_tool() -> str:\n",
    "    \"\"\"\n",
    "    A tool that can be used to scrape LinkedIn posts.\n",
    "    \"\"\"\n",
    "    return scrape_linkedin_posts_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -----------------------------\n",
    "##### LinkedIn Poster Tool\n",
    "##### -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedinPersonalFeedPoster:\n",
    "    \"\"\"\n",
    "    Posts text updates directly to a personal LinkedIn feed using the LinkedIn Marketing/Share APIs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, access_token: str):\n",
    "        self.access_token = access_token\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.access_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.user_id = self._get_user_id()\n",
    "\n",
    "    def _get_user_id(self) -> str:\n",
    "        \"\"\"\n",
    "        Calls the LinkedIn 'me' endpoint to retrieve and return the person's ID/URN.\n",
    "        \"\"\"\n",
    "        url = \"https://api.linkedin.com/v2/userinfo\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        print(\"User Info Response:\", response.text)\n",
    "        jsonData = json.loads(response.text)\n",
    "        return jsonData[\"sub\"]\n",
    "\n",
    "    def create_post(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Creates a post on the personal feed of the user whose access token we're using.\n",
    "        \"\"\"\n",
    "        url = \"https://api.linkedin.com/v2/ugcPosts\"\n",
    "\n",
    "        # A minimal text-only payload\n",
    "        payload = {\n",
    "            \"author\": f\"urn:li:person:{self.user_id}\",\n",
    "            \"lifecycleState\": \"PUBLISHED\",\n",
    "            \"specificContent\": {\n",
    "                \"com.linkedin.ugc.ShareContent\": {\n",
    "                    \"shareCommentary\": {\"text\": text},\n",
    "                    \"shareMediaCategory\": \"NONE\"\n",
    "                }\n",
    "            },\n",
    "            \"visibility\": {\n",
    "                \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        resp = requests.post(url, headers=self.headers, data=json.dumps(payload))\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, this is a test post from CrewAI!\"\n",
    "access_token = os.getenv(\"ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poster = LinkedinPersonalFeedPoster(access_token)\n",
    "result = poster.create_post(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"PostToLinkedInPersonalFeed\")\n",
    "def post_to_linkedin_personal_feed_tool(text: str) -> str:\n",
    "    \"\"\"\n",
    "    A tool that posts text to a personal LinkedIn feed. \n",
    "    The `access_token` is read from environment variables.\n",
    "    \"\"\"\n",
    "    access_token = Config.ACCESS_TOKEN\n",
    "    if not access_token:\n",
    "        raise ValueError(\"No ACCESS_TOKEN found in environment.\")\n",
    "\n",
    "    poster = LinkedinPersonalFeedPoster(access_token)\n",
    "    result = poster.create_post(text)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------\n",
    "#### LLM Setup\n",
    "#### -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = ChatOpenAI(api_key=Config.OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n",
    "mistral_llm = ChatMistralAI(api_key=Config.MISTRAL_API_KEY, model=\"mistral-large-latest\")\n",
    "\n",
    "# Example Tools\n",
    "from crewai_tools import ScrapeWebsiteTool, SerperDevTool\n",
    "scrape_website_tool = ScrapeWebsiteTool()\n",
    "search_tool = SerperDevTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------\n",
    "#### Agents & Tasks\n",
    "#### -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_scraper_agent = Agent(\n",
    "    role=\"LinkedIn Post Scraper\",\n",
    "    goal=\"Your goal is to scrape a LinkedIn profile to get a list of posts from the given profile\",\n",
    "    tools=[scrape_linkedin_posts_tool],\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You are an experienced programmer who excels at web scraping.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=openai_llm\n",
    ")\n",
    "\n",
    "# Task: Scrape LinkedIn\n",
    "scrape_linkedin_task = Task(\n",
    "    description=dedent(\"Scrape a LinkedIn profile to get some relevant posts.\"),\n",
    "    expected_output=dedent(\"A list of LinkedIn posts obtained from a LinkedIn profile.\"),\n",
    "    agent=linkedin_scraper_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_researcher_agent = Agent(\n",
    "    role=\"Web Researcher\",\n",
    "    goal=\"Your goal is to search for relevant content about the comparison between Llama 2 and Llama 3\",\n",
    "    tools=[scrape_website_tool, search_tool],\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You are proficient at searching for specific topics on the web, \n",
    "        selecting those that provide more value and information.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=openai_llm\n",
    ")\n",
    "\n",
    "# Task: Web research\n",
    "web_research_task = Task(\n",
    "    description=dedent(\"Get valuable and high quality web information about the comparison between Llama 2 and Llama 3.\"),\n",
    "    expected_output=dedent(\"High quality information about Llama 2 vs Llama 3.\"),\n",
    "    agent=web_researcher_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doppelganger_agent = Agent(\n",
    "    role=\"LinkedIn Post Creator\",\n",
    "    goal=\"You will create a LinkedIn post comparing Llama 2 and Llama 3 following the writing style observed in the LinkedIn posts scraped by the LinkedIn Post Scraper.\",\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You are an expert in writing LinkedIn posts replicating influencer style.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=mistral_llm\n",
    ")\n",
    "\n",
    "# Task: Create LinkedIn post based on scraped data + research\n",
    "create_linkedin_post_task = Task(\n",
    "    description=dedent(\n",
    "        \"Create a LinkedIn post comparing Llama 2 and Llama 3 following the style found in the scraped LinkedIn posts.\"\n",
    "    ),\n",
    "    expected_output=dedent(\n",
    "        \"A high-quality and engaging LinkedIn post comparing Llama 2 and Llama 3 \"\n",
    "        \"in the style of the scraped posts.\"\n",
    "    ),\n",
    "    agent=doppelganger_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_poster_agent = Agent(\n",
    "    role=\"LinkedIn Poster\",\n",
    "    # We do not reference {text} here because it's not defined yet\n",
    "    goal=\"Your goal is to take the final post text created by the Doppelganger agent and post it to LinkedIn.\",\n",
    "    # Pass the tool reference (function), not a function call\n",
    "    tools=[post_to_linkedin_personal_feed_tool],\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You are an intern who posts on behalf of the profile owner.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=openai_llm\n",
    ")\n",
    "\n",
    "# Task: Post the LinkedIn content from the Doppelganger agent\n",
    "post_linkedin_task = Task(\n",
    "    description=dedent(\n",
    "        \"\"\"\n",
    "        Take the final post from the Doppelganger agent and post it to LinkedIn \n",
    "        using the 'PostToLinkedInPersonalFeed' tool.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    expected_output=dedent(\"A confirmation that the LinkedIn post was successfully made.\"),\n",
    "    agent=linkedin_poster_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide context to the final post creation task (optional chaining)\n",
    "create_linkedin_post_task.context = [scrape_linkedin_task, web_research_task]\n",
    "# Also connect the final post creation to the posting task\n",
    "post_linkedin_task.context = [create_linkedin_post_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------\n",
    "#### Crew and Pipeline\n",
    "#### -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.telemetry import Telemetry\n",
    "\n",
    "def noop(*args, **kwargs):\n",
    "    print(\"Telemetry method called and noop'd\\n\")\n",
    "    pass\n",
    "\n",
    "for attr in dir(Telemetry):\n",
    "    if callable(getattr(Telemetry, attr)) and not attr.startswith(\"__\"):\n",
    "    setattr(Telemetry, attr, noop)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import SerperDevTool\n",
    "\n",
    "os.environ[\"OTEL_SDK_DISABLED\"] = \"true\"\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[\n",
    "        linkedin_scraper_agent,\n",
    "        web_researcher_agent,\n",
    "        doppelganger_agent,\n",
    "        linkedin_poster_agent\n",
    "    ],\n",
    "    tasks=[\n",
    "        scrape_linkedin_task,\n",
    "        web_research_task,\n",
    "        create_linkedin_post_task,\n",
    "        post_linkedin_task\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off the pipeline\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(\"Pipeline Result:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
