{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Influencer Project Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape LinkedIn Post: This Scrape AI Training Data From LinkedIn Profile for post Personalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Linked Scraper tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from crewai_tools import tool\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Config:\n",
    "#     \"\"\"\n",
    "#     A configuration class that fetches environment variables.\n",
    "#     \"\"\"\n",
    "#     OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#     MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "#     SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "#     LINKEDIN_EMAIL = os.getenv(\"LINKEDIN_EMAIL\")\n",
    "#     LINKEDIN_PASSWORD = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "#     LINKEDIN_PROFILE_NAME = os.getenv(\"LINKEDIN_PROFILE_NAME\")\n",
    "    \n",
    "#     ACCESS_TOKEN = os.getenv(\"ACCESS_TOKEN\")\n",
    "\n",
    "from config.configuration import Config\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if environment variables are loaded correctly\n",
    "print(\"OPENAI_API_KEY:\", Config.OPENAI_API_KEY)\n",
    "print(\"MISTRAL_API_KEY:\", Config.MISTRAL_API_KEY)\n",
    "print(\"SERPER_API_KEY:\", Config.SERPER_API_KEY)\n",
    "print(\"LINKEDIN_EMAIL:\", Config.LINKEDIN_EMAIL)\n",
    "print(\"LINKEDIN_PASSWORD:\", Config.LINKEDIN_PASSWORD)\n",
    "print(\"LINKEDIN_PROFILE_NAME:\", Config.LINKEDIN_PROFILE_NAME)\n",
    "print(\"ACCESS_TOKEN:\", Config.ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_content(page_source: str):\n",
    "    \"\"\"\n",
    "    Parses the page source HTML of a LinkedIn profile and filters \n",
    "    the containers that contain post information.\n",
    "    \"\"\"\n",
    "    linkedin_soup = BeautifulSoup(page_source.encode(\"utf-8\"), \"lxml\")\n",
    "    containers = linkedin_soup.find_all(\"div\", {\"class\": \"feed-shared-update-v2\"})\n",
    "    containers = [container for container in containers \n",
    "                  if 'activity' in container.get('data-urn', '')]\n",
    "    return containers\n",
    "\n",
    "def get_post_content(container, selector, attributes):\n",
    "    \"\"\"\n",
    "    Retrieves the text content from a specific HTML element \n",
    "    within a container.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = container.find(selector, attributes)\n",
    "        if element:\n",
    "            return element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting post content: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "def get_linkedin_posts(page_source: str):\n",
    "    \"\"\"\n",
    "    Uses parse_html_content to identify relevant containers, then \n",
    "    extracts the post text from each container.\n",
    "    \"\"\"\n",
    "    containers = parse_html_content(page_source)\n",
    "    posts = []\n",
    "    for container in containers:\n",
    "        post_content = get_post_content(container, \"div\", {\"class\": \"update-components-text\"})\n",
    "        posts.append(post_content)\n",
    "    return posts\n",
    "\n",
    "\n",
    "# from src.linkedIn_agent.utils.common import parse_html_content, get_post_content, get_linkedin_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedinToolException(Exception):\n",
    "    \"\"\"\n",
    "    Custom exception used when LinkedIn credentials are not provided in env variables.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\"You need to set the LINKEDIN_EMAIL and LINKEDIN_PASSWORD env variables\")\n",
    "        \n",
    "\n",
    "def scrape_linkedin_posts_fn() -> str:\n",
    "    \"\"\"\n",
    "    A function that logs into LinkedIn using credentials from environment \n",
    "    variables, scrolls through a profile's posts, and returns the posts.\n",
    "    \"\"\"\n",
    "    linkedin_username = os.environ.get(\"LINKEDIN_EMAIL\")\n",
    "    linkedin_password = os.environ.get(\"LINKEDIN_PASSWORD\")\n",
    "    linkedin_profile_name = os.environ.get(\"LINKEDIN_PROFILE_NAME\")\n",
    "\n",
    "    if not (linkedin_username and linkedin_password):\n",
    "        raise LinkedinToolException()\n",
    "\n",
    "    # Initialize WebDriver (make sure chromedriver is installed and in PATH)\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.linkedin.com/login\")\n",
    "\n",
    "    # Perform login\n",
    "    username_input = browser.find_element(\"id\", \"username\")\n",
    "    password_input = browser.find_element(\"id\", \"password\")\n",
    "    username_input.send_keys(linkedin_username)\n",
    "    password_input.send_keys(linkedin_password)\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Navigate to the profile's \"Recent Activity\"\n",
    "    browser.get(f\"https://www.linkedin.com/in/{linkedin_profile_name}/recent-activity/all/\")\n",
    "\n",
    "    # Scroll to load more posts\n",
    "    for _ in range(2):\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Extract posts\n",
    "    posts = get_linkedin_posts(browser.page_source)\n",
    "    browser.quit()\n",
    "    return str(posts[:5])\n",
    "\n",
    "@tool(\"ScrapeLinkedinPosts\")\n",
    "def scrape_linkedin_posts_tool() -> str:\n",
    "    \"\"\"\n",
    "    A tool that can be used to scrape LinkedIn posts.\n",
    "    \"\"\"\n",
    "    return scrape_linkedin_posts_fn()\n",
    "\n",
    "\n",
    "# from src.linkedIn_agent.tools.linkedIn_scraper import scrape_linkedin_posts_fn, scrape_linkedin_posts_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts = scrape_linkedin_posts_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from textwrap import dedent\n",
    "from crewai import Task\n",
    "from crewai import Agent\n",
    "from crewai_tools import ScrapeWebsiteTool, SerperDevTool\n",
    "from src.linkedIn_agent.tools.linkedIn_scraper import scrape_linkedin_posts_tool\n",
    "\n",
    "# If you have these custom modules:\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Get LLM API Keys from Config\n",
    "# openai_key = Config.OPENAI_API_KEY\n",
    "# mistral_key = Config.MISTRAL_API_KEY\n",
    "\n",
    "openai_llm = ChatOpenAI(api_key=Config.OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n",
    "mistral_llm = ChatMistralAI(api_key=os.environ.get(\"MISTRAL_API_KEY\"), model=\"mistral-large-latest\", streaming=False)\n",
    "\n",
    "# Initialize other tools\n",
    "scrape_website_tool = ScrapeWebsiteTool()\n",
    "search_tool = SerperDevTool()\n",
    "\n",
    "# Define the LinkedIn scraper agent\n",
    "linkedin_scraper_agent = Agent(\n",
    "    role=\"LinkedIn Post Scraper\",\n",
    "    goal=\"Your goal is to scrape a LinkedIn profile to get a list of posts from the given profile\",\n",
    "    tools=[scrape_linkedin_posts_tool],\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You are an experienced programmer who excels at web scraping.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=openai_llm\n",
    ")\n",
    "\n",
    "# Define the scraping task\n",
    "scrape_linkedin_task = Task(\n",
    "    description=dedent(\n",
    "        \"Scrape a LinkedIn profile to get some relevant posts\"\n",
    "    ),\n",
    "    expected_output=dedent(\n",
    "        \"A list of LinkedIn posts obtained from a LinkedIn profile\"\n",
    "    ),\n",
    "    agent=linkedin_scraper_agent,\n",
    ")\n",
    "\n",
    "# Define the web researcher agent\n",
    "web_researcher_agent = Agent(\n",
    "    role=\"Web Researcher\",\n",
    "    goal=\"Your goal is to search for relevant content about the comparison between Llama 2 and Llama 3\",\n",
    "    tools=[scrape_website_tool, search_tool],\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You are proficient at searching for specific topics on the web, \n",
    "        selecting those that provide more value and information.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=openai_llm\n",
    ")\n",
    "\n",
    "# Define the web research task\n",
    "web_research_task = Task(\n",
    "    description=dedent(\n",
    "        \"Get valuable and high quality web information about the comparison between Llama 2 and Llama 3\"\n",
    "    ),\n",
    "    expected_output=dedent(\n",
    "        \"Your task is to gather high quality information about the comparison between Llama 2 and Llama 3\"\n",
    "    ),\n",
    "    agent=web_researcher_agent,\n",
    ")\n",
    "\n",
    "# Define the doppelganger agent\n",
    "doppelganger_agent = Agent(\n",
    "    role=\"LinkedIn Post Creator\",\n",
    "    goal=\"You will create a LinkedIn post comparing Llama 2 and Llama 3 following the writing style observed in the LinkedIn posts scraped by the LinkedIn Post Scraper.\",\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You are an expert in writing LinkedIn posts replicating any influencer style.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=openai_llm\n",
    ")\n",
    "\n",
    "# Define the final post creation task\n",
    "create_linkedin_post_task = Task(\n",
    "    description=dedent(\n",
    "        \"Create a LinkedIn post comparing Llama 2 and Llama 3 following the writing-style expressed in the scraped LinkedIn posts.\"\n",
    "    ),\n",
    "    expected_output=dedent(\n",
    "        \"A high-quality and engaging LinkedIn post comparing Llama 2 and Llama 3. \"\n",
    "        \"The LinkedIn post must follow the same writing-style as the one expressed in the scraped LinkedIn posts.\"\n",
    "    ),\n",
    "    agent=doppelganger_agent,\n",
    ")\n",
    "\n",
    "# Provide context to the final post creation task\n",
    "create_linkedin_post_task.context = [scrape_linkedin_task, web_research_task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Crew\n",
    "\n",
    "# Create a Crew of agents and tasks\n",
    "crew = Crew(\n",
    "    agents=[\n",
    "        linkedin_scraper_agent,\n",
    "        web_researcher_agent,\n",
    "        doppelganger_agent,\n",
    "    ],\n",
    "    tasks=[\n",
    "        scrape_linkedin_task,\n",
    "        web_research_task,\n",
    "        create_linkedin_post_task,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Kick off the pipeline\n",
    "result = crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is the result: \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
